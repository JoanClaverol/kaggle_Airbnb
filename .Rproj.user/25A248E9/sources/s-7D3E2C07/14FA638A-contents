---
title: "Exploration_NY"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, echo = FALSE, message = FALSE, warning = FALSE)
```

# Data description 

* The following information is copy abnd pasted from the kaggle website in this [link](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data/kernels?sortBy=voteCount&group=everyone&pageSize=20&datasetId=268833). 

## Context

Since 2008, guests and hosts have used Airbnb to expand on traveling possibilities and present more unique, personalized way of experiencing the world. This dataset describes the listing activity and metrics in NYC, NY for 2019.

## Content
This data file includes all needed information to find out more about hosts, geographical availability, necessary metrics to make predictions and draw conclusions.

## Acknowledgements

This public dataset is part of Airbnb, and the original source can be found on this website.

## Inspiration

1. What can we learn about different hosts and areas?
2. What can we learn from predictions? (ex: locations, prices, reviews, etc)
3. Which hosts are the busiest and why?
4. Is there any noticeable difference of traffic among different areas and what could be the reason for it?

```{r libraries and data loading}
# libraries
if (require(pacman) == FALSE) {
  install.packages("pacman")
}
pacman::p_load(tidyverse, lubridate, magrittr,
               # styles to tables
               kableExtra, knitr,
               # visualizations
               highcharter, leaflet, corrplot,# ggplot included on tidyverse
               # working with text
               tidytext,
               # machine learning
               caret, modelr, fastDummies
               )
# load data
data <- read_csv(
  "../data/raw_data/new-york-city-airbnb-open-data/AB_NYC_2019.csv",
  col_types = cols()
  )
# define main used color
col <- "dodgerblue4"
# caption used for all the charts 
caption <- "Airbnb listings and metrics in NYC, NY, USA (2019)"
```

# Analysing hosts and areas

## Hosts analyisis

How many unique hosts do we have?

```{r}
kable(data %>% 
        distinct(host_id) %>% 
        count() %>% 
        rename("Number of unqiue hosts:" = n)) %>% 
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = F)
```

It seems there are hosts that has more than one apartment (listing ID). Let's find out which are the hosts with more apartmetns:

```{r}
data %>% 
  group_by(host_id) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  head(50) %>% 
  hchart(type = "bar", hcaes(x = reorder(host_id,n), y = n)) %>% 
  hc_colors(colors = "#16359b") %>% 
  hc_xAxis(title = list(text = "Host ID")) %>% 
  hc_yAxis(title = list(text = "Number of listings id")) %>% 
  hc_title(text = "Number of unqiue listing id by host ID")
```

There are 4 hosts IDs that has more than 100 unique IDs, probably they are part from a company (probably all the hostsi IDs bigger than 25 they are). Let's find out their names:

```{r}
treshold_listing_id <- 50
top_listingIDs <- data %>% 
  group_by(host_id) %>% 
  count() %>% 
  filter(n > treshold_listing_id) 
top_listingIDs %>% 
  left_join(y = data %>% select(host_id, host_name), by = "host_id") %>% 
  distinct(host_id, .keep_all = T) %>% 
  arrange(desc(n)) %>% 
  hchart(type = "bar", hcaes(x = host_name, y = n)) %>% 
  hc_colors(colors = "#16359b") %>% 
  hc_xAxis(title = list(text = "Host name")) %>% 
  hc_yAxis(title = list(text = "Number of listings id")) %>% 
  hc_title(text = paste("Zoom In on the hosts with more than", treshold_listing_id, "listing IDs"))
```

The question after that:

* Should I exclude these hosts from our analysis? Why?
* Can I found more information about them? Maybe I can place them into a map in order to find where they are.

Let's find out where Sonder (NYC) has major part of his apartments (*NOTE: maybe has some relation with the sonder placed in the 4th position, we can check that with the places that they are, but that will be a future step):

```{r}
data %>%
  select(host_name, neighbourhood, latitude, longitude) %>% 
  ggplot() +
    geom_point(aes(x = longitude, y = latitude), color = "grey") +
    geom_point(aes(x = if_else(host_name == "Sonder (NYC)", longitude, NULL), 
                   y = if_else(host_name == "Sonder (NYC)", latitude, NULL), 
                   color = if_else(host_name == "Sonder (NYC)", neighbourhood, NULL))) +
    scale_color_brewer(palette = "Dark2") +
    xlab(label = "longitude") + ylab(label = "latitude") + 
    labs(title = "Map of Airbnb apartments",
         subtitle = "Colors show where the host Sonder (NYC) is placed",
         caption = caption) +
    labs(color = "Neighbourhood:") +
    theme_minimal()
```

It seems this host is only focused on one small part of New York. We can check if this pattern is also common for the hosts with more than 50 listings ids:

```{r}
data %>%
  left_join(y = top_listingIDs, by = "host_id") %>% 
  select(n, host_name, neighbourhood, latitude, longitude) %>% 
  ggplot() +
    geom_point(aes(x = longitude, y = latitude), color = "grey") +
    geom_point(aes(x = if_else(!is.na(n), longitude, NULL), 
                   y = if_else(!is.na(n), latitude, NULL)), 
               color = "red", alpha = 0.3) +
    xlab(label = "longitude") + ylab(label = "latitude") + 
    labs(title = "Map of Airbnb apartments",
         subtitle = paste("The red points represents the appartments of the hosts with more than", 
                          treshold_listing_id, "appartments"),
         caption = caption) +
    labs(color = "Neighbourhood:") +
    theme_minimal()
```

The hypothesis seems to be correct. I can bet if we look for the places with a higher price, we will see a nice relation to this part of New York. To be able to compare the prices between different zones, we will differentiate the room types by colors:

```{r}
# plot creation
data %>%
  group_by(neighbourhood_group, room_type) %>% 
  summarise(mean_price = median(price, na.rm = T), n = n()) %>%
  arrange(desc(mean_price)) %>% 
  ggplot() +
    geom_point(aes(x = reorder(neighbourhood_group, mean_price), 
                   y = mean_price, size = n, color = room_type)) +
    coord_flip() + 
    ylab("Mean price in $") +
    labs(title = "Mean price by each neighbourhood group",
         subtitle = paste("The size of the points represents the quantity of",
                          "observations included\n(range from 9 to 13.000",
                          "obs.)", 
                          sep = " "),
         color = "Room type:", 
         caption = caption) +
    guides(color = guide_legend(order = 1), size = FALSE) +
    scale_color_brewer(palette = "Dark2") +
    theme_minimal() +
    theme(
      legend.position = "top",
      legend.title = element_blank(),
      axis.title.y = element_blank()
    )
```

These corporations know where to put their money. The price of Manhattans Aribnbs are almost the double than Brooklyn, in 2nd position. 

## Check bias in the data

Before we start with the model creation, we would like to know if we have enough information from all the areas to create a model representative enough for all of them. 

```{r}
# tranform neighbourhood to factors
data$neighbourhood_group <- factor(data$neighbourhood_group)
# define the function for giving colors to categorical values
fact_pal <- colorFactor(palette = "RdYlBu", data$neighbourhood_group) 
# creating the map
leaflet(data = data) %>% 
  addProviderTiles(provider = providers$CartoDB.Positron) %>%
  addCircles(lng = ~longitude, lat = ~latitude, 
             color = ~fact_pal(neighbourhood_group), opacity = 0.1) %>% 
  addLegend(position = "topleft", pal = fact_pal, 
            values = ~neighbourhood_group, title = "Neighbourhood group") %>% 
  setView(lng = -73.95568, lat = 40.72307, zoom = 10) %>% 
  addMiniMap()
```

The map above shows as the ditribution of the data based on group of neighbourhoods, but with a table it would be much clear:

```{r}
kable(
  data %>% 
    group_by(neighbourhood_group) %>% 
    summarise(n = n()) %>% 
    mutate("Perc. of the data" = paste(round((n/sum(n))*100,0), "%")) %>% 
    arrange(desc(n)) %>% 
    rename("Number of observations" = n)
  ) %>% 
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = F)
  
```

We can conlclude we have a clear bias on the data. *Bronx and Staten Island* are completly missrepresented, and *Manhattan and Brooklyn* overrepresented. We will have to look if we will have to apply some techniques to avoid a a bias on our model. 

# Creating the models

## Price prediction

As there is a variable telling there is a minimum number of nights you have to stay, we will create a new variable called **total_price** that considers that option:

```{r}
data %<>% 
  mutate(total_price = price * minimum_nights)
```



We can already mention that *location* and *room type* has a strong influence on price. We are going to create a linear model to evaluate the prices:

```{r}
# model creation 
mod_price <- lm(total_price ~., data = data %>% 
                  select(neighbourhood_group, room_type, total_price))
# checking at the metrics
data %>% 
  add_predictions(model = mod_price, var = "price_pred") %>% 
  add_residuals(model = mod_price, var = "price_resid") -> temp
postResample(pred = temp$price_pred, obs = temp$price)

```

Conclusion: bad results.

Why? Let's do an error check:

```{r}
temp %>% 
  ggplot() +
    geom_point(aes(x = price_pred, y = price)) +
    geom_abline(slope = 1, intercept = 0, color = "red") + 
    labs(title = "Error analyisis for price model",
         subtitle = "In a perfect situation, all the points should fall into the red line") +
    xlab("Price predictions") + ylab("Price") +
    theme_minimal() 
```

Okey, we can extract different conclusions froms this graph:

* Find a numeric variable that helps me to explain a bigger part of the price. 
* Our model tends to underpredict the price. we will have to check wwith a qqplot our errors distribution. 

```{r}
temp %>% 
  ggplot() +
    stat_qq(aes(sample = price_resid)) + 
    labs(title = "Residuals distribution to detect a tendency on the model\nto underpredict the price") +
    theme_minimal()
```

### 2nd round, which groups I am not explaining?


We will use a correlation matrix to find the correlated variables with the price:

```{r}
num_data <- data %>% 
  filter(neighbourhood_group == "Queens") %>% 
  select_if(is.numeric) %>% 
  select(-id, -host_id, -latitude, -longitude, -total_price) %>% 
  drop_na()

corrplot(corr = cor(num_data), order = "hclust", method = "pie", type = "lower", 
         tl.col = "black", sig.level = 0.2, tl.srt = 0.5)
```

No linear correlation with our numerical values. It's time to mine our data to extract the maximum amount of information. We will start by the name of the appartments.

#### Mining the names to extract insights

First, we will find the most used words in the names to create new features to predict the price (we have used the following [post](https://steemit.com/programming/@dkmathstats/finding-the-most-frequent-words-in-text-with-r) to extract them:

```{r}
text <- c(data$name)

# clean and extract 
text <- paste(text, collapse = " ")
text <- str_replace_all(text, pattern = '\"', replacement = "") # Remove slashes
text <- str_replace_all(text, pattern = '\n', replacement = "") # Remove \n
text <- str_replace_all(text, pattern = '\u0092', replacement = "'") #Replace with quote
text <- str_replace_all(text, pattern = '\u0091', replacement = "'") #Replace with quote
text_df <- tibble(Text = text) # tibble aka neater data frame

# unnest the information in the text
text_words <- text_df %>% 
  unnest_tokens(output = word, input = Text) 

# data(stop_words) # Stop words.
 
text_words  <- text_words  %>%
   anti_join(stop_words) # Remove stop words in peter_words Joining, by = "word"
text_wordcounts <- text_words %>% count(word, sort = TRUE)

# plot the results
text_wordcounts %>% 
  arrange(desc(n)) %>% 
  head(50) %>% 
  ggplot() +
    geom_col(aes(x = reorder(word, n), y = n), fill = col) +
    coord_flip() +
    labs(title = "Words frequency in the appartments name", 
         caption = caption) +
    ylab("Word frequency") +
    theme_minimal() +
    theme(axis.title.y = element_blank())
# define wanted words (repeated more than 25 times)
relevant_words <- text_wordcounts %>% 
  filter(n >= 50) %>% 
  select(word) 
```


We will decide to keep all the variables that appear more than 50 times:

```{r}
# create new feaures and create dummy variables
words_data <- data %>% 
  mutate(
    temp_name = str_replace_all(name, pattern = '\"', replacement = ""),
    temp_name = str_replace_all(temp_name, pattern = '\n', replacement = ""),
    temp_name = str_replace_all(temp_name, pattern = '\u0092', replacement = "'"),
    temp_name = str_replace_all(temp_name, pattern = '\u0091', replacement = "'")
  ) %>% 
  unnest_tokens(output = word, input = temp_name) %>% 
  anti_join(stop_words) %>%  # stop words extraction
  mutate(word_filter = if_else(word %in% relevant_words$word, word, "none")) %>% 
  dummy_cols(select_columns = "word_filter") %>% 
  distinct(id, .keep_all = T)

# regression model to predict price
mod_price_dummy <- lm(price ~., data = words_data %>% 
                        select(neighbourhood_group, room_type, price, 
                               starts_with("word_filter_")))

```



